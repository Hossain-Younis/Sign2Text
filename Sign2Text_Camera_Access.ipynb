{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "mount_file_id": "1cof4_7IyNAXnw5DjgWwQ7V5XapBmp60d",
      "authorship_tag": "ABX9TyNexZjkWJEVHXqLIeCdCyOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hossain-Younis/Sign2Text/blob/main/Sign2Text_Camera_Access.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WuAO5Bon4z5s"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giving the model access to the camera to live tranlate"
      ],
      "metadata": {
        "id": "X7VXpOCb5knC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Sign2Text\n",
        "model = load_model('/content/drive/MyDrive/Colab Notebooks/Sign2Text/Sign2Text.keras')\n",
        "\n",
        "# Creating a placeholder list for the labels on the model was trained on\n",
        "class_labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"del\", \"nothing\", \"space\"]\n",
        "\n",
        "# OpenCV window settings\n",
        "cv2.namedWindow('Sign2Text', cv2.WINDOW_NORMAL)\n",
        "cv2.resizeWindow('Sign2Text', 800, 600)\n",
        "\n",
        "# Setting up the camera\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Capturing frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Converting to grayscale\n",
        "    processed_frame = cv2.cvtColor(frame, 0)\n",
        "\n",
        "    # Resizing\n",
        "    processed_frame = cv2.resize(processed_frame, (32, 32))\n",
        "\n",
        "    # Normalizing pixel values to be between 0 and 1\n",
        "    processed_frame = processed_frame / 255.0\n",
        "\n",
        "    # Expanding dimensions to match the model's expected input dimensions\n",
        "    input_data = np.expand_dims(processed_frame, axis=0)\n",
        "\n",
        "    # Making predictions\n",
        "    predictions = model.predict(input_data)\n",
        "\n",
        "    # Getting the predicted class and confidence\n",
        "    predicted_class = np.argmax(predictions)\n",
        "    confidence = predictions[0][predicted_class]\n",
        "\n",
        "    # Getting the label from the placeholder list\n",
        "    predicted_label = class_labels[predicted_class]\n",
        "\n",
        "    # Displaying the result on the frame\n",
        "    cv2.putText(frame, f\"Predicted: {predicted_label}, Confidence: {confidence:.2f}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "    # Displaying the frame\n",
        "    cv2.imshow('Sign2Text', frame)\n",
        "\n",
        "    # Breaking the loop if the 'q' key is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Releasing the camera and close all OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "0xpPUe5e5nab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}